1.没有做类别的更新
2.没有做schema、本体的更新（本体里使用的词汇，表达概念的方式等）
3.查看百科的修改记录，看看百科是否提供更新流的信息
4.根据查询需求去做更新（用户查不到）
5.根据用户反馈去修改（用户标记某属性为错误）
6.利用HTTP头信息，“last_modified"去鉴定。（但不是所有页面都有该头信息，需要调研一下百科页面）
7.更新频率还不够高，不够实时（DBpedia之后时间在2-3分钟）
8.每天更新的量还比较小，且随着更新量增加，正确率在下降。（DBpedia每天更新100000个页面）
9.把一些一定不会被更新的数据排除在查询之外，提高效率。
10.没有和百科本身的动态性关联起来，而是根据热词去更新实体属性。
11.隔一段时间重新更新一遍整个知识库
12.没有实体、RDF三元组的删除操作
13.zhishi.me中只有简单的类别层次关系，没有严格的本体结构，这样导致了属性名、属性值等定义不统一，当百科页面里面属性的表达方式发生改变时，更新会有问题。
14.训练回归模型，特征选择不够合理，需要选择更适合百科的特征。
15.更新可以以实体为单位也可以以三元组为单位进行。
16.基本套路：1）使用热词（因为热词需要更新的可能性更大） 2）通过估算实体以往的更新频率去爬段哪些实体更可能需要更新
             3）预测实体或三元组的未来生命周期           4）通过源数据提供的更新流
             5）通过last_modified标识
17.要根据数据的动态性进行更新，关键是如何用模型刻画实体或三元组的动态性。
18.肖的论文只对热词的语义相关词进行优先级排序，没有针对整个知识库，这更新的不全面。
19.利用知识库snapshots进行训练得到一个回归模型，对每个实体或三元组进行优先级排序。重点是特征值的设计和选取。
20.百度百科的changelogs怎么获得（肖的论文里有提到）。应该是每一个百科页面都有一个changelog
21.有了Hot词之后，如何进一步扩展。
22.通过页面过去snapshots的改变率去预测它未来的改变率。
23.按顺序依次访问所有实体（每天访问一部分）
24.根据Pagerank排序所有实体。
25.可用特征：age（从上一次被更新到现在的时间）、PageRank、Size(三元组总数)、ChangeRatio(两次更新之间变换的三元组数量的差值)、ChangeRate(两次之间的变化率（变化值除以时间）)、Dynamics（一段时间内的变换率之和）以及肖的论文中设计的特征也可以作为参考。
  Dynamics不管在单轮更新还是多轮更新中都表现最好。
26.更新分两部分：1.知识库已有的实体进行更新
                 2.发现知识库中没有的知识
27.HTTP metadata monitoring:analysis of http response headers including datestamp and ETag.
28.email notification "www.changedetection.com"
29.类似于rdf:type这类的三元组可能相对静态，而一些敏感数据可能相对动态，可以从这个角度对RDF三元组作区分。
30.更新有两种选择：1）主动去检查是否改变（pull)。 2）被动接受订阅通知(push)  
31.区分实体、属性类型，静态属性和动态属性。
32. 并不是实体过去的改变频率越高，它需要被更新的频率越高。这要根据本地知识库的freshness评价方法来确定。相关研究表明，他们之间的关系曲线是一个拱形。 